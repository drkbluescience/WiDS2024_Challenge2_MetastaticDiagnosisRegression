{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "62B8jYwg_Zyq",
        "Pn4QJ-fap1tF",
        "dJ3pPcTpqOFp",
        "NWTtpmkaNuDa",
        "G4_U2629aq1k"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1i-KRyXrMJtAuS7K0uSMNxcgyXWs3t6h7/view?usp=sharing)\n",
        "\n",
        "#**Imputation Techniques for Missing Data Handling**"
      ],
      "metadata": {
        "id": "Qz6cALqk_dt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`1.` Introduction**"
      ],
      "metadata": {
        "id": "62B8jYwg_Zyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is focused on handling missing data in a dataset using various imputation techniques. In particular, both standard imputation methods and group-based imputation strategies are employed to fill in missing values. The main objectives are:\n",
        "\n",
        "- **Identifying Missing Values:** Understanding the scope and nature of missing data in both numerical and categorical features.\n",
        "- **Choosing Imputation Methods:** Applying different imputation techniques, including group-based mean/mode strategies and specific imputer objects.\n",
        "- **Combining Techniques:** Utilizing a flexible approach to handle missing values in a way that maximizes data quality for downstream analysis.\n",
        "- **Evaluation of Imputation Impact:** Comparing the effects of different imputation strategies on model performance and overall data integrity.\n",
        "\n",
        "**Data Preprocessing**\n",
        "\n",
        "Before testing each combination of imputers on the model, several preprocessing steps were implemented to ensure the dataset was properly prepared for analysis.\n",
        "1. **Encode Categorical Features:** Categorical variables were encoded using techniques such as classify_cardinality or direct dummy encoding, as appropriate.\n",
        "2. **Replace Missing Values (Imputation):** Various imputation techniques were applied to fill in the missing values in both numerical and categorical features.\n",
        "3. **Scale Data: Numerical features were scaled using three options:** Standard scaling, MinMax scaling, and Robust scaling, to ensure that they are appropriately represented for model training.\n",
        "\n",
        "The dataset under analysis contains both numerical and categorical features with missing values, and the goal is to make these features suitable for machine learning models by ensuring no missing data remains. Group-level aggregation is also utilized for imputation, where features are grouped based on categorical and numerical columns to apply targeted filling strategies."
      ],
      "metadata": {
        "id": "gUcdDTfy_J9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`2.` Imputation Techniques Implementation**"
      ],
      "metadata": {
        "id": "Pn4QJ-fap1tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6BiOLSLIrTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60dea88-602b-407f-83fd-cdc955664522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "!pip install dask[dataframe] --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from itertools import product\n",
        "\n",
        "!pip install matplotlib --upgrade --quiet\n",
        "from matplotlib import rcParams\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "!pip install catboost --quiet\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import (GradientBoostingRegressor, ExtraTreesRegressor,\n",
        "                              RandomForestRegressor,\n",
        "                              AdaBoostRegressor)\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up\n",
        "seed = 42  # Random seed for reproducibility\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set Matplotlib defaults\n",
        "plt.style.use('fivethirtyeight')\n",
        "rcParams['figure.figsize'] = [5,3]\n",
        "rcParams.update({'font.size': 8})\n",
        "\n",
        "# Define Imputation Strategies\n",
        "# Global variables\n",
        "simple_cat = SimpleImputer(strategy='most_frequent')\n",
        "constant_cat = SimpleImputer(strategy='constant', fill_value='unknown')\n",
        "simple_mean_imputer = SimpleImputer(strategy='mean')\n",
        "simple_median_imputer = SimpleImputer(strategy='median')\n",
        "iterative_num_imputer = IterativeImputer(random_state=seed)\n",
        "\n",
        "NaN_models = ['CatBoost']\n",
        "\n",
        "categorical_imputers = {\n",
        "    'Simple Categorical': simple_cat,\n",
        "    'Constant Categorical': constant_cat,\n",
        "    'Group Mode': None,\n",
        "}\n",
        "\n",
        "# Encapsulate the functionality for data imputation and model evaluation\n",
        "class DataImputer:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        # Initialize imputer settings and model constructors\n",
        "        self.categorical_imputers = categorical_imputers\n",
        "        self.numerical_imputers = numerical_imputers = {\n",
        "            'Simple Mean': simple_mean_imputer,\n",
        "            'Simple Median': simple_median_imputer,\n",
        "            'KNN': KNNImputer(n_neighbors=n_neighbors),\n",
        "            'No Num Imputation': None,\n",
        "            'Group Mean': None,\n",
        "            'Iterative': iterative_num_imputer\n",
        "            }\n",
        "\n",
        "        self.NaN_models = NaN_models\n",
        "\n",
        "        # Model names and corresponding constructor functions\n",
        "        self.model_constructors = {\n",
        "            'CatBoost': lambda **kwargs: CatBoostRegressor(cat_features=kwargs.get('cat_features', None), random_state=seed, verbose=0),\n",
        "            'LGBM': lambda **kwargs: LGBMRegressor(random_state=seed, force_col_wise=True, verbose=-1, **kwargs),\n",
        "            'GradientBoosting': lambda **kwargs: GradientBoostingRegressor(random_state=seed, **kwargs),\n",
        "            'ExtraTrees': lambda **kwargs: ExtraTreesRegressor(random_state=seed, **kwargs),\n",
        "            'XGB': lambda **kwargs: XGBRegressor(random_state=seed, **kwargs),\n",
        "            'AdaBoost': lambda **kwargs: AdaBoostRegressor(DecisionTreeRegressor(**kwargs), random_state=seed),  # Base estimator: DecisionTree\n",
        "            'DecisionTree': lambda **kwargs: DecisionTreeRegressor(random_state=seed, **kwargs),\n",
        "            'RandomForest': lambda **kwargs: RandomForestRegressor(random_state=seed, **kwargs),\n",
        "            'MLPRegressor': lambda **kwargs: MLPRegressor(random_state=seed, **kwargs),\n",
        "            'Ridge': lambda **kwargs: Ridge(random_state=seed, **kwargs),\n",
        "            'Lasso': lambda **kwargs: Lasso(random_state=seed, **kwargs),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize_imputation_scores(rmses, stds, labels, model_name):\n",
        "        \"\"\"\n",
        "        Plot RMSE scores for different imputation techniques.\n",
        "        \"\"\"\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6, 8))\n",
        "\n",
        "        colors = plt.cm.Paired(np.linspace(0, 1, len(rmses)))\n",
        "        y_positions = np.arange(len(rmses))\n",
        "\n",
        "        ax.barh(\n",
        "            y_positions,\n",
        "            rmses,\n",
        "            xerr=stds,\n",
        "            color=colors,\n",
        "            alpha=0.6,\n",
        "            align=\"center\"\n",
        "        )\n",
        "\n",
        "        ax.set_title(f\"{model_name} Model Scores with Respect to Imputation Techniques\")\n",
        "        ax.set_xlabel(\"RMSE\")\n",
        "        ax.set_yticks(y_positions)\n",
        "        ax.set_yticklabels(labels)\n",
        "        ax.invert_yaxis()  # Highest RMSE on top\n",
        "\n",
        "        ax.set_xlim(left=np.min(rmses) * 0.9, right=np.max(rmses) * 1.1)\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def cat_num_features(df):\n",
        "        \"\"\"\n",
        "        Identify categorical and numerical features in the DataFrame.\n",
        "        \"\"\"\n",
        "\n",
        "        cat_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        num_features = df.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
        "        return cat_features, num_features\n",
        "\n",
        "    @staticmethod\n",
        "    def classify_cardinality(X, low_thrs=20, high_thrs=50):\n",
        "        \"\"\"\n",
        "        Classify categorical features based on their cardinality into low, medium, and high.\n",
        "        \"\"\"\n",
        "\n",
        "        low_card, med_card, high_card = [], [], []\n",
        "        cat_features, num_features = DataImputer.cat_num_features(X)\n",
        "\n",
        "        for col in cat_features:\n",
        "            unique_count = X[col].nunique()\n",
        "\n",
        "            if unique_count < low_thrs:\n",
        "                low_card.append(col)\n",
        "            elif low_thrs <= unique_count <= high_thrs:\n",
        "                med_card.append(col)\n",
        "            else:\n",
        "                high_card.append(col)\n",
        "\n",
        "        return low_card, med_card, high_card\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_data_cardinality(X, X_test=None, drop_first=False, apply_cardinality=True, low_thrs=20, high_thrs=50):\n",
        "        \"\"\"\n",
        "        Encode data based on cardinality, using dummy encoding for low cardinality features and\n",
        "        label encoding for medium and high cardinality features.\n",
        "        \"\"\"\n",
        "        # Check if X is a pandas DataFrame\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            raise TypeError(\"X must be a pandas DataFrame.\")\n",
        "\n",
        "        X_encoded = X.copy()\n",
        "        X_test_encoded = X_test.copy() if X_test is not None else None\n",
        "\n",
        "        if apply_cardinality:\n",
        "            # classifies features based on their cardinality\n",
        "            low_card, med_card, high_card = DataImputer.classify_cardinality(X, low_thrs=low_thrs, high_thrs=high_thrs)\n",
        "\n",
        "            # Dummy encoding for low cardinality features\n",
        "            X_encoded = pd.get_dummies(X, columns=low_card, drop_first=drop_first)\n",
        "            X_encoded = X_encoded.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "            if X_test_encoded is not None:\n",
        "                X_test_encoded = pd.get_dummies(X_test_encoded, columns=low_card, drop_first=drop_first)\n",
        "                X_test_encoded = X_test_encoded.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "                # Align test data with train data\n",
        "                X_encoded, X_test_encoded = X_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "            # Label encoding for medium and high cardinality features\n",
        "            med_high = med_card + high_card\n",
        "            if len(med_high) > 0:\n",
        "                for col in med_high:\n",
        "                    le = LabelEncoder()\n",
        "                    X_encoded[col] = le.fit_transform(X_encoded[col])\n",
        "\n",
        "                    # Encode test data if available\n",
        "                    if X_test_encoded is not None:\n",
        "                        try:\n",
        "                            X_test_encoded[col] = le.transform(X_test_encoded[col])\n",
        "                        except ValueError as e:\n",
        "                            X_test_encoded[col] = X_test_encoded[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
        "        else:\n",
        "            # Dummy encoding for all categorical features\n",
        "            X_encoded = pd.get_dummies(X, drop_first=drop_first)\n",
        "            X_encoded = X_encoded.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "            # Encode test data if available\n",
        "            if X_test_encoded is not None:\n",
        "                X_test_encoded = pd.get_dummies(X_test_encoded, drop_first=drop_first)\n",
        "                X_test_encoded = X_test_encoded.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "                # Set of features by aligning their columns\n",
        "                X_encoded, X_test_encoded = X_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "        if X_test_encoded is not None:\n",
        "            return X_encoded, X_test_encoded\n",
        "        else:\n",
        "            return X_encoded\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_data(X_train, X_test=None, scale_type='Standard'):\n",
        "        \"\"\"\n",
        "        Scale data using StandardScaler, MinMaxScaler, or RobustScaler.\n",
        "        \"\"\"\n",
        "\n",
        "        if scale_type == 'Standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif scale_type == 'MinMax':\n",
        "            scaler = MinMaxScaler()\n",
        "        elif scale_type == 'Robust':\n",
        "            scaler = RobustScaler()\n",
        "        else:\n",
        "            raise ValueError(f\"'{scale_type}' is not a valid scaling type.\")\n",
        "\n",
        "        if not isinstance(X_train, pd.DataFrame):\n",
        "            raise TypeError(\"X_train must be a pandas DataFrame.\")\n",
        "\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "\n",
        "        if X_test is not None:\n",
        "            if not isinstance(X_test, pd.DataFrame):\n",
        "                raise TypeError(\"X_test  must be a pandas DataFrame.\")\n",
        "\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "            X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "            return X_train_scaled, X_test_scaled\n",
        "\n",
        "        return X_train_scaled\n",
        "\n",
        "    @staticmethod\n",
        "    def rmse_score(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the Root Mean Squared Error (RMSE).\n",
        "        \"\"\"\n",
        "        if len(y_true) != len(y_pred):\n",
        "            raise ValueError(\"Input arrays must have the same length.\")\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_val_score_func(model, X, y, cv=3):\n",
        "        \"\"\"\n",
        "        Perform cross-validation and return RMSE scores.\n",
        "        \"\"\"\n",
        "\n",
        "        rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
        "        scores = cross_val_score(model, X, y, scoring=rmse_scorer, cv=cv, n_jobs=-1)\n",
        "        return scores\n",
        "\n",
        "    def get_regressor(self, model_name, cat_features=None, params=None):\n",
        "        \"\"\"\n",
        "        Retrieve a regression model based on the model name.\n",
        "        \"\"\"\n",
        "\n",
        "        # Check if model name is valid\n",
        "        if model_name not in self.model_constructors:\n",
        "            raise ValueError(f\"'{model_name}' is not a valid model.\")\n",
        "\n",
        "        # Check cat_features in CatBoost model\n",
        "        if model_name == 'CatBoost':\n",
        "            if cat_features is None:\n",
        "                raise ValueError(\"CatBoostRegressor requires 'cat_features' to be specified.\")\n",
        "            else:\n",
        "                params = params or {}  # Create an empty dict if params is None\n",
        "                params['cat_features'] = cat_features  # Add cat_features to params\n",
        "\n",
        "        # Seçilen modeli döndür\n",
        "        return self.model_constructors[model_name](**(params if params is not None else {}))\n",
        "\n",
        "    @staticmethod\n",
        "    def group_imputation(df_org, group_cols_cat, group_cols_num, strategy='mean'):\n",
        "        \"\"\"\n",
        "        Impute missing values based on group statistics (mean or mode).\n",
        "        \"\"\"\n",
        "        df = df_org.copy()\n",
        "        if strategy == 'mean':\n",
        "            for group_col in group_cols_num:\n",
        "                for feature in group_col['features']:\n",
        "                        df[feature] = df.groupby(group_col['group_cols'])[feature].transform(\n",
        "                            lambda x: x.fillna(x.mean() if x.mean() == x.mean() else x))\n",
        "\n",
        "        elif strategy == 'mode':\n",
        "            for group_col in group_cols_cat:\n",
        "                for feature in group_col['features']:\n",
        "                        df[feature] = df.groupby(group_col['group_cols'])[feature].transform(\n",
        "                            lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'unknown'))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported imputation strategy: {strategy}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_imputation(X, features, imputer, group_cols_cat, group_cols_num, strategy=None, X_test=None):\n",
        "        \"\"\"\n",
        "        Apply imputation to the specified features.\n",
        "        \"\"\"\n",
        "        if imputer:\n",
        "            if X_test is not None:\n",
        "                return imputer.fit_transform(X[features]), imputer.transform(X_test[features])\n",
        "            else:\n",
        "                return imputer.fit_transform(X[features]), None\n",
        "\n",
        "        elif strategy:\n",
        "            if X_test is not None:\n",
        "                return DataImputer.group_imputation(X, group_cols_cat, group_cols_num, strategy=strategy), DataImputer.group_imputation(X_test, group_cols_cat, group_cols_num, strategy=strategy)\n",
        "            else:\n",
        "                return DataImputer.group_imputation(X, group_cols_cat, group_cols_num, strategy=strategy), None\n",
        "\n",
        "        return X[features], X_test[features] if X_test is not None else None\n",
        "\n",
        "    @staticmethod\n",
        "    def impute(X, cat_name, num_name, cat_imputer, num_imputer, num_features, cat_features, missing_numerical, missing_categorical, group_cols_cat, group_cols_num, apply_cardinality=True, X_test=None):\n",
        "        \"\"\"\n",
        "        Perform imputation for both numerical and categorical features.\n",
        "        \"\"\"\n",
        "        X_num_imputed, X_test_num_imputed = DataImputer.apply_imputation(X,\n",
        "                                                            num_features,\n",
        "                                                            num_imputer,\n",
        "                                                            group_cols_cat,\n",
        "                                                            group_cols_num,\n",
        "                                                            strategy='mean' if num_name == 'Group Mean' else None,\n",
        "                                                            X_test=X_test)\n",
        "        X_num_imputed = pd.DataFrame(X_num_imputed, columns=num_features)\n",
        "\n",
        "        X_cat_imputed, X_test_cat_imputed = DataImputer.apply_imputation(\n",
        "            X,\n",
        "            cat_features,\n",
        "            cat_imputer,\n",
        "            group_cols_cat,\n",
        "            group_cols_num,\n",
        "            strategy='mode' if cat_name == 'Group Mode' else None,\n",
        "            X_test=X_test)\n",
        "\n",
        "\n",
        "        X_cat_imputed = pd.DataFrame(X_cat_imputed, columns=cat_features)\n",
        "\n",
        "        X_imputed = pd.concat([X_num_imputed.reset_index(drop=True), X_cat_imputed.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        if X_test is not None:\n",
        "            X_test_num_imputed = pd.DataFrame(X_test_num_imputed, columns=num_features)\n",
        "            X_test_cat_imputed = pd.DataFrame(X_test_cat_imputed, columns=cat_features)\n",
        "            X_test_imputed = pd.concat([X_test_num_imputed.reset_index(drop=True), X_test_cat_imputed.reset_index(drop=True)], axis=1)\n",
        "\n",
        "            return X_imputed, X_test_imputed\n",
        "\n",
        "        return X_imputed, None\n",
        "\n",
        "    def imputation_model_scores(self, X, y, model_name, missing_categorical, missing_numerical, group_cols_cat, group_cols_num, cv, imputers=None, visualize=False, apply_cardinality=True, X_test=None):\n",
        "        \"\"\"\n",
        "        Evaluate different imputation techniques using a specific model.\n",
        "        \"\"\"\n",
        "        # Check if model name is valid\n",
        "        if model_name not in self.model_constructors:\n",
        "            raise ValueError(f\"'{model_name}' is not a valid model.\")\n",
        "\n",
        "        # Get categorical and numerical features\n",
        "        cat_features, num_features = DataImputer.cat_num_features(X)\n",
        "        # Check for CatBoostRegressor\n",
        "        if 'CatBoost' in model_name:\n",
        "            if cat_features is None or len(cat_features) == 0:\n",
        "                raise ValueError(\"CatBoostRegressor requires 'cat_features' to be specified and cannot be empty.\")\n",
        "            regressor = self.get_regressor(model_name, cat_features)\n",
        "        else:\n",
        "            regressor = self.get_regressor(model_name)\n",
        "\n",
        "        rmses, stds, x_labels = [], [], []\n",
        "        # imputation_scores = []\n",
        "        best_train_imputed = {}\n",
        "        best_test_imputed = {}\n",
        "\n",
        "        # Combination of various techniques\n",
        "        for (num_name, num_imputer), (cat_name, cat_imputer) in product(self.numerical_imputers.items(), self.categorical_imputers.items()):\n",
        "            if imputers is not None:\n",
        "                if num_name not in imputers or cat_name not in imputers:\n",
        "                    continue\n",
        "\n",
        "            # Group Mean and Group Mode controls\n",
        "            if num_name == 'Group Mean' and group_cols_num is None:\n",
        "                print(\"Warning: 'group_cols_num' None. Group-based imputation will not be possible.\")\n",
        "                continue\n",
        "\n",
        "            if cat_name == 'Group Mode' and group_cols_cat is None:\n",
        "                print(\" Warning: 'group_cols_cat' None. Group-based imputation will not be possible.\")\n",
        "                continue\n",
        "\n",
        "            if model_name not in self.NaN_models:\n",
        "                if 'Group' in num_name:\n",
        "                    continue\n",
        "                if 'No' in num_name:\n",
        "                    continue\n",
        "\n",
        "            imputation_name = f'{cat_name} and {num_name}'\n",
        "\n",
        "            # Perform the imputation process\n",
        "            X_imputed, X_test_imputed = DataImputer.impute(\n",
        "                X,\n",
        "                cat_name,\n",
        "                num_name,\n",
        "                cat_imputer,\n",
        "                num_imputer,\n",
        "                num_features,\n",
        "                cat_features,\n",
        "                missing_numerical,\n",
        "                missing_categorical,\n",
        "                group_cols_cat,\n",
        "                group_cols_num,\n",
        "                apply_cardinality=apply_cardinality,\n",
        "                X_test=X_test)\n",
        "\n",
        "            # Missing value check\n",
        "            missing_in_train = X_imputed.isnull().values.any()\n",
        "            missing_in_test = X_test_imputed is not None and X_test_imputed.isnull().values.any()\n",
        "\n",
        "            if missing_in_train or missing_in_test:\n",
        "                # raise ValueError(f\"There are still missing values in the DataFrame with {imputation_name} for {model_name}.\")\n",
        "                print(f\"Warning: {imputation_name} imputation resulted in missing values for {model_name}.\")\n",
        "\n",
        "                # Skip the loop if the model is not CatBoost\n",
        "                if 'CatBoost' not in model_name:\n",
        "                    continue\n",
        "\n",
        "            best_train_imputed[imputation_name] = X_imputed\n",
        "            best_test_imputed[imputation_name] = X_test_imputed\n",
        "\n",
        "            # If the model is not CatBoost and there are no missing values, apply encoding\n",
        "            if 'CatBoost' not in model_name:\n",
        "                if X_test is not None:\n",
        "                    X_imputed, X_test_imputed = DataImputer.encode_data_cardinality(\n",
        "                        X_imputed,\n",
        "                        X_test= X_test_imputed,\n",
        "                        drop_first=True,\n",
        "                        apply_cardinality=apply_cardinality)\n",
        "                else:\n",
        "                    X_imputed = DataImputer.encode_data_cardinality(\n",
        "                        X_imputed,\n",
        "                        drop_first=True,\n",
        "                        apply_cardinality=apply_cardinality)\n",
        "\n",
        "            # Scale the data if needed\n",
        "            if model_name in ['MLPRegressor', 'Ridge', 'Lasso']:\n",
        "                X_imputed = DataImputer.scale_data(X_imputed)\n",
        "\n",
        "            # Split the data\n",
        "            if cv == 1:\n",
        "                X_train, X_val, y_train, y_val = train_test_split(\n",
        "                    X_imputed,\n",
        "                    y,\n",
        "                    test_size=.2,\n",
        "                    random_state=seed)\n",
        "\n",
        "                regressor.fit(X_train, y_train)\n",
        "\n",
        "                y_pred = regressor.predict(X_val)\n",
        "                rmse = DataImputer.rmse_score(y_val, y_pred)\n",
        "                # print(f\"RMSE: {rmse:.4f}\\n\")\n",
        "\n",
        "                rmses.append(rmse)\n",
        "                stds.append(0)\n",
        "                # imputation_scores.append({'methods': imputation_name,'RMSE': rmse, 'Std': 0})\n",
        "            else:\n",
        "                scores = DataImputer.cross_val_score_func(regressor, X_imputed, y, cv=cv)\n",
        "                # print(f\"RMSE: {scores.mean():.4f}, Std: {scores.std():.4f}\\n\")\n",
        "                rmses.append(scores.mean())\n",
        "                stds.append(scores.std())\n",
        "                # imputation_scores.append({'methods': imputation_name,'RMSE': scores.mean(), 'Std': scores.std()})\n",
        "\n",
        "            x_labels.append(imputation_name)\n",
        "\n",
        "        # Sort the results in ascending order\n",
        "        sorted_indices = np.argsort(rmses)\n",
        "        sorted_rmses = np.array(rmses)[sorted_indices]\n",
        "        sorted_stds = np.array(stds)[sorted_indices]\n",
        "        sorted_labels = np.array(x_labels)[sorted_indices]\n",
        "\n",
        "        best_index = np.argmin(sorted_rmses)\n",
        "        best_method = sorted_labels[best_index]\n",
        "        best_rmse, best_std = sorted_rmses[best_index], sorted_stds[best_index]\n",
        "\n",
        "        best_train = best_train_imputed[best_method]\n",
        "        best_test = best_test_imputed[best_method]\n",
        "\n",
        "        if cv == 1:\n",
        "            print(f\"The best method for {model_name}: {best_method}, RMSE: {best_rmse:.4f}\")\n",
        "        else:\n",
        "            print(f\"The best method for {model_name}: {best_method}, RMSE: {best_rmse:.4f}, Std: {best_std:.4f}\")\n",
        "\n",
        "        if visualize:\n",
        "            DataImputer.visualize_imputation_scores(sorted_rmses, sorted_stds, sorted_labels, model_name)\n",
        "\n",
        "        return best_train, best_test, best_method, best_rmse, best_std\n",
        "\n",
        "\n",
        "    def impute_and_evaluate_models(self, X, y, models, missing_categorical, missing_numerical, group_cols_cat=None, group_cols_num=None, imputers=None, visualize_first=False, visualize=False, cv=1, apply_cardinality=True, X_test=None):\n",
        "        \"\"\"\n",
        "        Impute missing values and evaluate different models.\n",
        "        \"\"\"\n",
        "        print(\"---START---\")\n",
        "        best_data = []\n",
        "\n",
        "        for i, model_name in enumerate(models, 1):\n",
        "            t1 = datetime.now()\n",
        "\n",
        "            # Visualization for the first model\n",
        "            if visualize_first and i == 1:\n",
        "                train_imputed, test_imputed, best_method, best_rmse, best_std = self.imputation_model_scores(\n",
        "                    X,\n",
        "                    y,\n",
        "                    model_name,\n",
        "                    missing_categorical,\n",
        "                    missing_numerical,\n",
        "                    group_cols_cat,\n",
        "                    group_cols_num,\n",
        "                    cv,\n",
        "                    imputers=imputers,\n",
        "                    visualize=True,\n",
        "                    apply_cardinality=True,\n",
        "                    X_test=X_test)\n",
        "            else:\n",
        "                train_imputed, test_imputed, best_method, best_rmse, best_std = self.imputation_model_scores(\n",
        "                    X,\n",
        "                    y,\n",
        "                    model_name,\n",
        "                    missing_categorical,\n",
        "                    missing_numerical,\n",
        "                    group_cols_cat,\n",
        "                    group_cols_num,\n",
        "                    cv,\n",
        "                    imputers=imputers,\n",
        "                    visualize=visualize,\n",
        "                    apply_cardinality=True,\n",
        "                    X_test=X_test)\n",
        "\n",
        "            best_data.append({'model_name': model_name, 'train': train_imputed, 'test': test_imputed, 'best_method': best_method, 'best_rmse':best_rmse, 'best_std':best_std})\n",
        "\n",
        "            t2 = datetime.now()\n",
        "            time_diff = t2 - t1\n",
        "            print(f\"Time taken: {time_diff}\\n\")\n",
        "        print(\"---END---\")\n",
        "        return best_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**`3.` Conclusion**"
      ],
      "metadata": {
        "id": "dJ3pPcTpqOFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this analysis,  a variety of imputation techniques were employed to address missing data in both numerical and categorical features. The results underscore the importance of choosing an imputation strategy tailored to the dataset's characteristics, as the method used can significantly influence the quality of predictions.\n",
        "\n",
        "Group-based imputation methods, such as mean and mode imputation, were found to be particularly effective, enhancing data completeness while maintaining the relationships between features. This approach was particularly beneficial for tree-based models like the GradientBoostingRegressor, which leverage group similarities to improve predictive accuracy. Standard imputation methods, such as KNN and Simple Mean, also performed well under specific circumstances, showcasing their value in filling missing values.\n",
        "\n",
        "To better understand the impact of KNN imputation, a 9-fold cross-validation was performed for n_neighbors values of 3, 5, and 7, similar to the approach by Sebastian Jäger and colleagues **(Jäger et al., 2021)**. This allowed for a detailed observation of how changes in the hyperparameter affect model performance. The results are given in the table below.\n",
        "\n",
        "|Model|Imputation Methods|n\\_splits|RMSE|\n",
        "|---|---|---|---|\n",
        "|CatBoostRegressor|Group Mode and KNN|3|83\\.5513|\n",
        "|CatBoostRegressor|Constant Categorical and KNN|5|83\\.3924|\n",
        "|CatBoostRegressor|Group Mode and KNN|7|83\\.405|\n",
        "|LGBMRegressor|Constant Categorical and KNN|3|83\\.6108|\n",
        "|LGBMRegressor|Simple Categorical and KNN|5|84\\.0352|\n",
        "|LGBMRegressor|Constant Categorical and KNN|7|83\\.9112|\n",
        "|GradientBoostingRegressor|Group Mode and KNN|3|82\\.9256|\n",
        "|GradientBoostingRegressor|Group Mode and KNN|5|82\\.8678|\n",
        "|GradientBoostingRegressor|Group Mode and KNN|7|82\\.787|\n",
        "\n",
        "The evaluation was conducted using both train-test split and cross-validation techniques, demonstrating that the choice of imputation method has a direct impact on model performance. For instance, KNN imputation, despite being frequently noted in literature for its effectiveness in predicting missing categorical data **(Memon, Wamala, & Kabano, 2023)**, was primarily applied here to numerical data, showing its versatility. This process not only ensured that no missing values remained in the dataset but also allowed for a clear assessment of how different models respond to various imputation strategies. The methods and RMSE results for each model showing the best performance, based on the train-test split, are presented in the table below.\n",
        "\n",
        "|Model|Imputation Methods|RMSE|\n",
        "|---|---|---|\n",
        "|GradientBoostingRegressor|Group Mode and KNN|82\\.8678|\n",
        "|CatBoostRegressor|Constant Categorical and KNN|83\\.3924|\n",
        "|LGBMRegressor|Constant Categorical and Simple Mean|83\\.8968|\n",
        "|XGBoost|Constant Categorical and Simple Median|87\\.4577|\n",
        "|RandomForestRegressor|Constant Categorical and Simple Median|87\\.6868|\n",
        "|MLPRegressor|Simple Categorical and Simple Mean|88\\.6755|\n",
        "|ExtraTreesRegressor|Constant Categorical and KNN|89\\.0288|\n",
        "|AdaBoostRegressor|Constant Categorical and Simple Median|93\\.4804|\n",
        "|DecisionTreeRegressor|Group Mode and Simple Mean|119\\.4029|\n",
        "\n",
        "The key takeaway from this study is that combining standard and group-based imputation methods can yield optimal results, particularly for datasets with a diverse range of missing data types. The performance metrics, such as RMSE, highlighted in this analysis provide a benchmark for choosing suitable imputation techniques in future machine learning projects.\n"
      ],
      "metadata": {
        "id": "T78cjfv0hZDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`4.` Contribution and Collaboration**"
      ],
      "metadata": {
        "id": "NWTtpmkaNuDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook serves as a resource for analyzing imputation techniques for missing data handling. It can be applied to various datasets beyond the current scope. Feedback and contributions from those interested in enhancing this work are welcome. If you have suggestions, improvements, or additional techniques to share, please feel free to reach out!"
      ],
      "metadata": {
        "id": "DBUZPIqjO3oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**`5.` References**"
      ],
      "metadata": {
        "id": "G4_U2629aq1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Memon, S. M. Z., Wamala, R., & Kabano, I. H. (2023). A comparison of imputation methods for categorical data. *Informatics in Medicine Unlocked*, 42, 101382. https://doi.org/10.1016/j.imu.2023.101382\n",
        "\n",
        "- Jäger, S., Allhorn, A., & Bießmann, F. (2021). A Benchmark for Data Imputation Methods. *Frontiers in Big Data*, 4. https://doi.org/10.3389/fdata.2021.693674\n"
      ],
      "metadata": {
        "id": "_CXzt-RWjhaD"
      }
    }
  ]
}